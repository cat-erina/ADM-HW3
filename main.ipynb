{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 - What movie to watch tonight?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data collection task is divided into **three** subtasks.\n",
    "* Get the list of movies\n",
    "* Crawl Wikipedia\n",
    "* Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import of librabries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from bs4 import Tag, NavigableString, BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the list of movies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_page = 'https://raw.githubusercontent.com/CriMenghini/ADM/master/2019/Homework_3/data/movies1.html'\n",
    "big_page_response = requests.get(big_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_page_soup = BeautifulSoup(big_page_response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawl Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List_url=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in big_page_soup.find_all('tr')[1:]:\n",
    "    url = i.find_all('a')[0].get(\"href\") # get the url in the second column of each row starting from the sec. row\n",
    "    page_response= requests.get(url)  # reponse to the request\n",
    "    \n",
    "    if page_response.status_code == 200:  # 200 == means everthing is ok\n",
    "        with open(os.path.join('/Users/yves/Desktop/Data_Science/first_year/first_semester/adm/adm_hw3/html_files1',\"movie\"+str(counter+1)+\".html\"), \"w\") as file:\n",
    "            page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
    "            file.write(str(page_soup))  # writes the html page\n",
    "            List_url.append(url) # add url to the list L\n",
    "        counter += 1\n",
    "    elif page_response.status_code == 429: # 429 == means that your doing too many requaets\n",
    "        print('we must wait...')\n",
    "        time.sleep(600) # wait 10 mins \n",
    "        page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
    "        with open(os.path.join('/Users/yves/Desktop/Data_Science/first_year/first_semester/adm/adm_hw3/html_files1',\"movie\"+str(counter+1)+\".html\"), \"w\") as file:\n",
    "            file.write(str(page_soup))  # writes the html page\n",
    "            List_url.append(url) # add url to the list L\n",
    "        counter += 1\n",
    "    elif counter%25==0:  # every 25 files, wait 10 seconds\n",
    "        time.sleep(10)\n",
    "        page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
    "        with open(os.path.join('/Users/yves/Desktop/Data_Science/first_year/first_semester/adm/adm_hw3/html_files1',\"movie\"+str(counter+1)+\".html\"), \"w\") as file:\n",
    "            file.write(str(page_soup))  # writes the html page\n",
    "            List_url.append(url) # add url to the list L\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "    print(\"parsed movie \" + str(counter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse Downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "stop = set(stopwords.words('english'))\n",
    "Vocabulary = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function is to clean the rows of the intobox\n",
    "def clean_col(string):\n",
    "    clean1 = string.strip('\\n') # remove all escape characters\n",
    "    pattern = r'\\[.*?\\]'  # remove all square brakets and content\n",
    "    clean2 = re.sub(pattern, '', clean1)\n",
    "    return(clean2)\n",
    "\n",
    "# The following function is to extract info from the intobox\n",
    "\n",
    "# This function takes three inputs:\n",
    "# The first input is the html page - from which the info is going to be extracted.\n",
    "# The second input is a number associated to the page (This number is an identificator number, \n",
    "# thanks to which we will be able to retrieve the url of the html page).\n",
    "# The third input is the name given to the html page.\n",
    "\n",
    "def info_tsv(film_soup, movie_number, filename):  \n",
    "    \n",
    "    # This is the list of the info we want to take from the infobox\n",
    "    infb = ['Directed by', 'Produced by', 'Written by', 'Screenplay by',\n",
    "           'Story by', 'Based on', 'Starring', 'Narrated by', 'Music by',\n",
    "           'Cinematography', 'Edited by', 'Production company', \n",
    "            'Distributed by', 'Release date', 'Running time', 'Country',\n",
    "           'Language', 'Budget'] \n",
    "    \n",
    "    # remove all square brakets and content\n",
    "    pattern = r'\\[.*?\\]' \n",
    "    \n",
    "    # This list will be a list of list, and each list inside will contain the infobox related to each row, the Into and Plot\n",
    "    L = []  \n",
    "        \n",
    "    # Retrieve the Title   \n",
    "        \n",
    "    Initial_Title = film_soup.select(\"#firstHeading\")[0].text\n",
    "    L.append(['Title', str(Initial_Title)])\n",
    "    \n",
    "    # Retrieve the Intro\n",
    "     \n",
    "    contents = film_soup.findAll('div', attrs={'id': 'toc'}) \n",
    "    if len(contents) > 0:\n",
    "        Intr = contents[0].fetchPreviousSiblings('p') # give us all the paragraphs from the end of the first section\n",
    "        # These paragraphs correspond to the Intro\n",
    "        Intro = []\n",
    "        if len(Intr) > 0:\n",
    "            for p in reversed(Intr):  # we order the paragraphs as they should be (same order as in the original html page)\n",
    "                if len(p.text) > 5:  # If a paragraph has less than five strings we do not consider it to be a paragraph\n",
    "                    Intro.append(p.text)  # The treshold len(p.text) > 5 help us to remove all empty and meaningless paragraphs\n",
    "                else:\n",
    "                    continue\n",
    "            Intro = ''.join(Intro)\n",
    "            Final_intro = re.sub(pattern, '', Intro)  # remove all the square brakets and what it contains, because  \n",
    "                                                      # These brakets are Wikipedia notes/citations\n",
    "            if len(Final_intro)>20: # If a section, in this case the Intro, has less than 20 strings, we do consider it to be meaningless\n",
    "                L.append(['Intro', Final_intro.strip()])  # we save the info we've got from the intro\n",
    "            else:\n",
    "                L.append(['Intro', 'NA'])  # in case we don't have any info, we put an 'NA'\n",
    "    \n",
    "    \n",
    "    # Retrieve the Plot\n",
    "    \n",
    "    plot = []\n",
    "    \n",
    "    # Before starting looking for the info in the plot, we created a list of the most used names to indicate the section Plot\n",
    "    possibles = ['Plot','Synopsis','Plot synopsis','Plot summary', 'Plot_summary', 'Plot_synopsis'\n",
    "                 'Story','Plotline','The Beginning','Summary',\n",
    "                'Content','Premise', 'Intro', 'Intro']\n",
    "    for i in possibles:\n",
    "        items = film_soup.find(id=i) # se find the section with name == ith element in possibles\n",
    "        if items is None or len(items) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            # walk through the siblings of the parent (H2) node \n",
    "            # until we reach the next H2 node\n",
    "            for element in items.parent.nextSiblingGenerator():  # we basically take all the paragraphs till the end of the section\n",
    "                if element.name == \"h2\":\n",
    "                    break\n",
    "                if hasattr(element, \"text\"):\n",
    "                    plot.append(element.text)\n",
    "    plot = \"\".join(plot)\n",
    "    # remove all square brakets and content\n",
    "    Final_plot = re.sub(pattern, '', plot)\n",
    "    if len(Final_plot) > 20:  # same as before (for the Intro), in case the whole section has less than 20 string, we considered it meaningless\n",
    "        L.append(['Plot', Final_plot.strip()])  # we save the info found in the Plot\n",
    "    else:\n",
    "        L.append(['Plot', 'NA']) # in case we don't have any info, we put an 'NA'\n",
    "    \n",
    "    # Retrieve info in the infobox\n",
    "    \n",
    "    # we have found the infobox by not only looking for the tables in the page but by section the table with class = 'infobox vevent'  \n",
    "    if len(film_soup.findAll('table', {'class': 'infobox vevent'}))>0:  \n",
    "        \n",
    "        # If the box is not empty we go through it\n",
    "        if len(film_soup.findAll('table', {'class': 'infobox vevent'})[0].tbody.findAll('tr'))>0:\n",
    "            \n",
    "            # from the infobox the firt row correspond to the Name of the Film\n",
    "            Title = film_soup.findAll('table', {'class': 'infobox vevent'})[0].tbody.findAll('tr')[0].text\n",
    "            \n",
    "            # If a title contains more than 75 strings we do not consider it to a title, because it could be some additional \n",
    "            # messages inserted in the pages\n",
    "            if len(Title)>75:\n",
    "                \n",
    "                L.append(['Name', 'NA']) # in case we don't have any info, we put an 'NA'\n",
    "            else:\n",
    "                L.append(['Name', Title]) # we save the Name of the film\n",
    "            \n",
    "        else:\n",
    "            L.append(['Name', 'NA']) # in case we don't have any info, we put an 'NA'\n",
    "    \n",
    "    \n",
    "    # In the following table we will put all the info that we've retreived from the infobox - except the name.\n",
    "    # The the idea, is to do an intersection between the elements found and the elements we wanted to found\n",
    "    # for all the missing elements we will put 'NA'\n",
    "    \n",
    "    in_table = [] # This list will contain all the info found in the infobox\n",
    "    \n",
    "    # check if the table is not empty\n",
    "    if len(film_soup.findAll('table', {'class': 'infobox vevent'}))>0: \n",
    "        \n",
    "        # check if the row is not empty\n",
    "        if len(film_soup.findAll('table', {'class': 'infobox vevent'})[0].tbody.findAll('tr'))>0:\n",
    "            \n",
    "            #go through the columns of each row\n",
    "            for row in film_soup.findAll('table', {'class': 'infobox vevent'})[0].tbody.findAll('tr')[1:]:\n",
    "                \n",
    "                # check that the first column is not empty\n",
    "                first_col = row.findAll('th') \n",
    "                if len(first_col)>0:\n",
    "                    \n",
    "                    # extract and check the second column\n",
    "                    second_col = row.findAll('td')\n",
    "                    if len(second_col)>0:\n",
    "                        \n",
    "                        # external rows can contain internal rows. \n",
    "                        # extract elements in each row \n",
    "                        second_col = []\n",
    "                        for element in row.findAll('td')[0]:\n",
    "                            # check if the element in the row has a child\n",
    "                            if isinstance(element, Tag):\n",
    "                                second_col.append(element.text) # collect all the info found in this row\n",
    "                            else:\n",
    "                                second_col.append(element)\n",
    "                        second_col = ' '.join(second_col)\n",
    "                        \n",
    "                        new_second_col = clean_col(second_col) # clean the output -calling the initial function\n",
    "\n",
    "                        in_table.append(first_col[0].text) # save the feature of the row ex. 'Directed by'\n",
    "                        in_table.append(new_second_col)  # save the info corresponding to this feature\n",
    "                        \n",
    "\n",
    "            # put 'NA' to all missing values\n",
    "            for i in infb:\n",
    "                if i in in_table:\n",
    "                    index = in_table.index(i)\n",
    "                    L.append([i, in_table[index +1]])\n",
    "                else:\n",
    "                    L.append([i, 'NA']) # in case we don't have any info, we put an 'NA'\n",
    "    L.append(['Url', List_url[int(movie_number) -1]])  # from the list of the urls we take url of this html page\n",
    "    return(L) # we return all the info we need to save in the tsv files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of the tsv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = '/Users/yves/Desktop/Data_Science/first_year/first_semester/adm/adm_hw3/html_files/'\n",
    "\n",
    "    \n",
    "for filename in os.listdir(paths):\n",
    "    if filename not in movies_done:\n",
    "        if filename.endswith(\".html\"):\n",
    "            ff = open(paths + filename, 'r') # open the HTML page\n",
    "            html = ff.read()\n",
    "            film = BeautifulSoup(html)\n",
    "\n",
    "            page_name = filename.rstrip('.html') # extract the name of the file without extension and we will use this as the file id            \n",
    "            movie_number = re.findall('\\d+', filename )\n",
    "            movie_number = movie_number[0]\n",
    "            \n",
    "            \n",
    "            #save the info into tsv files\n",
    "            with open('/Users/yves/Desktop/Data_Science/first_year/first_semester/adm/adm_hw3/tsv_files/'+page_name+'.tsv', 'wt') as out_file:\n",
    "                tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "                if info_tsv(film,movie_number , filename) is None or len(info_tsv(film, movie_number, filename)) == 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    for col in info_tsv(film, movie_number, filename):\n",
    "                        tsv_writer.writerow([col[0], col[1]])\n",
    "            print('tsv created '+ str(len(movies_done)))\n",
    "\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "    else:\n",
    "        continue\n",
    "print('DONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "import itertools\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from itertools import chain\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity    \n",
    "import operator\n",
    "from nltk.corpus import wordnet as wn\n",
    "import math\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "attr = ['Intro', 'Plot']\n",
    "stop = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/yves/Desktop/Data_Science/first_year/first_semester/adm/adm_hw3/tsv_files/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your index!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part has been done in the previous part, directly while creating the tsv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_ict = {}\n",
    "Vocabulary = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "attr = ['Plot', 'Intro', 'Directed by', 'Produced by', 'Written by', 'Screenplay by',\n",
    "           'Story by', 'Based on', 'Starring', 'Narrated by', 'Music by',\n",
    "           'Cinematography', 'Edited by', 'Production company', \n",
    "            'Distributed by',  'Country',\n",
    "           'Language']\n",
    "\n",
    "i = 0\n",
    "for filename in os.listdir(path):  \n",
    "    if filename.endswith(\".tsv\"):\n",
    "        page_name = filename.rstrip('.tsv') # without extension\n",
    "        movie_number = re.findall('\\d+', filename )\n",
    "        movie_number = movie_number[0]\n",
    "        words = []\n",
    "        with open(path+filename) as fd:\n",
    "            rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "            for row in rd:\n",
    "                if row[0] in attr:\n",
    "                    for word in row[1].strip().split():\n",
    "                        if word == 'NA':\n",
    "                            continue\n",
    "                        else:\n",
    "                            word_cl1 = re.sub(r'[^\\w\\s]','',word)\n",
    "                            word_cl1 = word_cl1.lower()\n",
    "                            if word_cl1 not in stop:\n",
    "                                stemmed_word = ps.stem(word_cl1)\n",
    "                                words.append(stemmed_word)\n",
    "        words = list(set(words))\n",
    "        for clean_word in words:\n",
    "            if clean_word not in Vocabulary:\n",
    "                Vocabulary.append(clean_word)\n",
    "            else:\n",
    "                continue\n",
    "        for clean_word in words: \n",
    "            term_id = Vocabulary.index(clean_word)\n",
    "            if str(term_id) not in good_dict.keys():\n",
    "                good_dict[str(term_id)] = [filename]\n",
    "            else:\n",
    "                if filename in good_dict[str(term_id)]:\n",
    "                    continue\n",
    "                else:\n",
    "                    good_dict[str(term_id)].append(filename)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the dictionary and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/yves/Desktop/dictionary1.json', 'w') as outfile:\n",
    "    json.dump(good_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/yves/Desktop/Vocabulary.txt', 'w') as f:\n",
    "    for item in Vocabulary:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Initial dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =  open('/Users/yves/Desktop/dictionary1.json', 'r')\n",
    "good_dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vocabulary = open(\"/Users/yves/Desktop/Vocabulary.txt\", \"r\", \n",
    "                       encoding = \"utf-8\")          \n",
    "Vocabulary = Vocabulary.read()      \n",
    "Vocabulary = Vocabulary.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Execute the query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load your query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter string:  love movie\n"
     ]
    }
   ],
   "source": [
    "initial_query = input(\"Enter string:  \")\n",
    "initial_query = initial_query.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of the documents containg all the words in the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for stemming \n",
    "def stem_word(lst_words):\n",
    "    clean_lst_words= []\n",
    "    for word in lst_words:\n",
    "        word = ps.stem(word)\n",
    "        clean_lst_words.append(word)\n",
    "    clean_lst_words = list(set(clean_lst_words))\n",
    "    return(clean_lst_words)\n",
    "\n",
    "final_query = []\n",
    "for word in initial_query:\n",
    "    for ss in wn.synsets(word):\n",
    "        final_query.append((ss.lemma_names()[0]))\n",
    "        \n",
    "query = stem_word(final_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "for word in query:\n",
    "    if word in Vocabulary:\n",
    "        term_id = Vocabulary.index(word)\n",
    "        docs = good_dict[str(term_id)]\n",
    "        if docs is None or len(docs) == 0:\n",
    "            continue\n",
    "        else:\n",
    "            for i in docs:\n",
    "                lst.append(i)\n",
    "result=[]\n",
    "for i in lst:\n",
    "    if lst.count(i) >= len(initial_query)-1:\n",
    "         result.append(i)\n",
    "result = list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fired Up!</td>\n",
       "      <td>Fired Up! is a 2009 American teen comedy film ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Fired_Up!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Going the Distance</td>\n",
       "      <td>Going the Distance is a 2010 American romantic...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Going_the_Distan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scott Pilgrim vs. the World</td>\n",
       "      <td>Scott Pilgrim vs. the World is a 2010 action c...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Scott_Pilgrim_vs...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An Invisible Sign</td>\n",
       "      <td>An Invisible Sign is a 2010 American drama fil...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/An_Invisible_Sign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Blue Valentine</td>\n",
       "      <td>Blue Valentine  is a 2010 American romantic dr...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Blue_Valentine_(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Name  \\\n",
       "0                    Fired Up!   \n",
       "1           Going the Distance   \n",
       "2  Scott Pilgrim vs. the World   \n",
       "3            An Invisible Sign   \n",
       "4               Blue Valentine   \n",
       "\n",
       "                                               Intro  \\\n",
       "0  Fired Up! is a 2009 American teen comedy film ...   \n",
       "1  Going the Distance is a 2010 American romantic...   \n",
       "2  Scott Pilgrim vs. the World is a 2010 action c...   \n",
       "3  An Invisible Sign is a 2010 American drama fil...   \n",
       "4  Blue Valentine  is a 2010 American romantic dr...   \n",
       "\n",
       "                                                 Url  \n",
       "0            https://en.wikipedia.org/wiki/Fired_Up!  \n",
       "1  https://en.wikipedia.org/wiki/Going_the_Distan...  \n",
       "2  https://en.wikipedia.org/wiki/Scott_Pilgrim_vs...  \n",
       "3    https://en.wikipedia.org/wiki/An_Invisible_Sign  \n",
       "4  https://en.wikipedia.org/wiki/Blue_Valentine_(...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_df = []\n",
    "for movie in result:\n",
    "    element_in_movie = {}\n",
    "    with open(path+movie) as fd:\n",
    "        rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "        for row in rd:\n",
    "            if row[0] == 'Name':\n",
    "                element_in_movie['Name'] = row[1]\n",
    "            elif row[0] == 'Intro':\n",
    "                element_in_movie['Intro'] = row[1]\n",
    "            elif row[0] == 'Url':\n",
    "                element_in_movie['Url'] = row[1]\n",
    "    to_df.append(element_in_movie)\n",
    "df = pd.DataFrame(to_df)\n",
    "df = df.reindex(('Name', 'Intro', 'Url'), axis=1)\n",
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted index\n",
    "\n",
    "\n",
    "<p>\n",
    "    TF = (Frequency of the word in the sentence) / (Total number of words in the sentence)\n",
    "</p>\n",
    "<p>\n",
    "    IDF: (Total number of sentences (documents))/(Number of sentences (documents) containing the word)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = ['Intro', 'Plot']  \n",
    "ps = PorterStemmer()\n",
    "stop = set(stopwords.words('english'))\n",
    "lst = list(good_dict.keys())\n",
    "len_dict = len(os.listdir(path))\n",
    "New_Dict = {}\n",
    "\n",
    "for term_id in good_dict.keys():  # for each word in the dictionary\n",
    "    val_word = list(good_dict[term_id]) # we save the number of documents having that word\n",
    "    for doc in good_dict[term_id]:  # we go through each document\n",
    "        file = [] # here we will save the the words contained in the document\n",
    "        with open(path+doc) as fd:\n",
    "            rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "            for row in rd:\n",
    "                if row[0] in attr:\n",
    "                    for word in row[1].strip().split():\n",
    "                        if word == 'NA':\n",
    "                            continue\n",
    "                        else:\n",
    "                            word_cl1 = re.sub(r'[^\\w\\s]','',word)\n",
    "                            word_cl1 = word_cl1.lower()\n",
    "                            if word_cl1 not in stop:\n",
    "                                stemmed_word = ps.stem(word_cl1)\n",
    "                                file.append(stemmed_word)\n",
    "        if len(file) < 1:\n",
    "            if word in New_Dict.keys():  # add the word to the new dictioary if the word is not already inside and add the new value if the word is already inside\n",
    "                New_Dict[word].append((doc, 0))\n",
    "            else:\n",
    "                New_Dict[word] = [(doc, 0)]\n",
    "        else:\n",
    "            len_file = len(file) # the total number of words\n",
    "            word = Vocabulary[int(term_id)]\n",
    "            count = file.count(word) # the number of times that word is repeated in the document\n",
    "            tf = round(count/len_file, 4) # the frequency of the word in the document\n",
    "            idf = np.log(len_dict/len(val_word)) # the IDF \n",
    "            if term_id in New_Dict.keys():  # add the word to the new dictioary if the word is not already inside and add the new value if the word is already inside\n",
    "                New_Dict[term_id].append((doc, tf*idf))\n",
    "            else:\n",
    "                New_Dict[term_id] = [(doc, tf*idf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### save the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/yves/Desktop/tfidf.json', 'w') as outfile:\n",
    "    json.dump(New_Dict, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load the inverted index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =  open('/Users/yves/Desktop/partial_DICT_tfidf.json', 'r')\n",
    "New_Dict = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_query(query):\n",
    "    tf_idf = {}\n",
    "    len_query = len(query)\n",
    "    len_dict = len(os.listdir(path))\n",
    "    DF = {}\n",
    "    for word in query:# its a count funtion\n",
    "        if word not in DF.keys():\n",
    "            DF[word] = 1 \n",
    "        else:\n",
    "            DF[word]+=1\n",
    "    \n",
    "    for word in query:\n",
    "        \n",
    "        # TF = (Frequency of the word in the sentence) / (Total number of words in the sentence)\n",
    "        count = query.count(word)\n",
    "        freq = count/len_query\n",
    "        tf = freq\n",
    "        \n",
    "        val_word = DF[word]\n",
    "        idf = idf = np.log(len_dict/val_word)\n",
    "        if word in Vocabulary:\n",
    "            term_id = Vocabulary.index(word)\n",
    "            if word in tf_idf.keys():\n",
    "                tf_idf[str(term_id)]+= tf*idf\n",
    "            else:\n",
    "                tf_idf[str(term_id)] = tf*idf\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_query = tf_idf_query(query) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization, Cosine Similarity and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for i in range(len(result)):\n",
    "    small_dict = {}\n",
    "    for word in New_Dict.keys():\n",
    "        for files in New_Dict[word]:\n",
    "            if files[0] == result[i]:\n",
    "                small_dict[word] = files[1]\n",
    "    result_list.append(small_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    \n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_result = {}\n",
    "for i in range(len(result)):\n",
    "    cosine = get_cosine(tf_idf_query, result_list[i])\n",
    "    cos_result[result[i]] = cosine*100\n",
    "    \n",
    "cos_result = sorted(cos_result.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When in Rome</td>\n",
       "      <td>When in Rome is a 2010 American romantic comed...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/When_in_Rome_(20...</td>\n",
       "      <td>6.835857966671183 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear John</td>\n",
       "      <td>Dear John is a 2010 American romantic drama-wa...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dear_John_(2010_...</td>\n",
       "      <td>5.704633376565299 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I Love You Phillip Morris</td>\n",
       "      <td>I Love You Phillip Morris is a 2009 black come...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/I_Love_You_Phill...</td>\n",
       "      <td>5.264481089373219 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My One and Only</td>\n",
       "      <td>My One and Only is a 2009 comedy-drama film lo...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/My_One_and_Only_...</td>\n",
       "      <td>4.689575381407523 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Never Let Me Go</td>\n",
       "      <td>Never Let Me Go is a 2010 British dystopian ro...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Never_Let_Me_Go_...</td>\n",
       "      <td>4.209033222443061 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Eat Pray Love</td>\n",
       "      <td>Eat Pray Love is a 2010 American biographical ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Eat_Pray_Love</td>\n",
       "      <td>3.918970377109158 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I Can Do Bad All by Myself</td>\n",
       "      <td>I Can Do Bad All by Myself is a 2009 romantic ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/I_Can_Do_Bad_All...</td>\n",
       "      <td>3.623062757513399 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Informers</td>\n",
       "      <td>The Informers is a 2008 American ensemble Holl...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Informers_(2...</td>\n",
       "      <td>3.2380426575731334 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Limits of Control</td>\n",
       "      <td>The Limits of Control is a 2009 American film ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Limits_of_Co...</td>\n",
       "      <td>3.1846058945767446 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hurricane Season</td>\n",
       "      <td>Hurricane Season is a 2010 sports drama film d...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Hurricane_Season...</td>\n",
       "      <td>3.1793814369053632 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name  \\\n",
       "0                When in Rome   \n",
       "1                   Dear John   \n",
       "2   I Love You Phillip Morris   \n",
       "3             My One and Only   \n",
       "4             Never Let Me Go   \n",
       "5               Eat Pray Love   \n",
       "6  I Can Do Bad All by Myself   \n",
       "7               The Informers   \n",
       "8       The Limits of Control   \n",
       "9            Hurricane Season   \n",
       "\n",
       "                                               Intro  \\\n",
       "0  When in Rome is a 2010 American romantic comed...   \n",
       "1  Dear John is a 2010 American romantic drama-wa...   \n",
       "2  I Love You Phillip Morris is a 2009 black come...   \n",
       "3  My One and Only is a 2009 comedy-drama film lo...   \n",
       "4  Never Let Me Go is a 2010 British dystopian ro...   \n",
       "5  Eat Pray Love is a 2010 American biographical ...   \n",
       "6  I Can Do Bad All by Myself is a 2009 romantic ...   \n",
       "7  The Informers is a 2008 American ensemble Holl...   \n",
       "8  The Limits of Control is a 2009 American film ...   \n",
       "9  Hurricane Season is a 2010 sports drama film d...   \n",
       "\n",
       "                                                 Url            Similarity  \n",
       "0  https://en.wikipedia.org/wiki/When_in_Rome_(20...   6.835857966671183 %  \n",
       "1  https://en.wikipedia.org/wiki/Dear_John_(2010_...   5.704633376565299 %  \n",
       "2  https://en.wikipedia.org/wiki/I_Love_You_Phill...   5.264481089373219 %  \n",
       "3  https://en.wikipedia.org/wiki/My_One_and_Only_...   4.689575381407523 %  \n",
       "4  https://en.wikipedia.org/wiki/Never_Let_Me_Go_...   4.209033222443061 %  \n",
       "5        https://en.wikipedia.org/wiki/Eat_Pray_Love   3.918970377109158 %  \n",
       "6  https://en.wikipedia.org/wiki/I_Can_Do_Bad_All...   3.623062757513399 %  \n",
       "7  https://en.wikipedia.org/wiki/The_Informers_(2...  3.2380426575731334 %  \n",
       "8  https://en.wikipedia.org/wiki/The_Limits_of_Co...  3.1846058945767446 %  \n",
       "9  https://en.wikipedia.org/wiki/Hurricane_Season...  3.1793814369053632 %  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_df = []\n",
    "for movie in cos_result:\n",
    "    element_in_movie = {}\n",
    "    with open(path+movie[0]) as fd:\n",
    "        rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "        for row in rd:\n",
    "            if row[0] == 'Name':\n",
    "                element_in_movie['Name'] = row[1]\n",
    "            elif row[0] == 'Intro':\n",
    "                element_in_movie['Intro'] = row[1]\n",
    "            elif row[0] == 'Url':\n",
    "                element_in_movie['Url'] = row[1]\n",
    "        element_in_movie['Similarity'] = str(movie[1])+' %'\n",
    "    to_df.append(element_in_movie)\n",
    "df = pd.DataFrame(to_df)\n",
    "df = df.reindex(('Name', 'Intro', 'Url', 'Similarity'), axis=1)\n",
    "df.iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New inverted matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = []\n",
    "ps = PorterStemmer()\n",
    "stop = set(stopwords.words('english'))\n",
    "len_dict = len(os.listdir(path))\n",
    "attr = ['Plot', 'Intro']\n",
    "important = ['Directed by', 'Produced by', 'Written by', 'Screenplay by',\n",
    "           'Story by', 'Based on', 'Starring', 'Narrated by', 'Music by',\n",
    "           'Cinematography', 'Edited by', 'Production company', \n",
    "            'Distributed by',  'Country',\n",
    "           'Language']\n",
    "\n",
    "New_Dict = {}\n",
    "for term_id in good_dict.keys():\n",
    "    val_word = list(good_dict[term_id])\n",
    "    for doc in good_dict[term_id]:\n",
    "\n",
    "        intro_plot = []\n",
    "        important1 =[]\n",
    "        title = []\n",
    "        with open(path+doc) as fd:\n",
    "            rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "            for row in rd:\n",
    "                if row[0] in attr:\n",
    "                    for word in row[1].strip().split():\n",
    "                        if word == 'NA':\n",
    "                            continue\n",
    "                        else:\n",
    "                            word_cl1 = re.sub(r'[^\\w\\s]','',word)\n",
    "                            word_cl1 = word_cl1.lower()\n",
    "                            if word_cl1 not in stop:\n",
    "                                stemmed_word = ps.stem(word_cl1)\n",
    "                                intro_plot.append(stemmed_word)\n",
    "                elif row[0] in important:\n",
    "                    for word in row[1].strip().split():\n",
    "                        if word == 'NA':\n",
    "                            continue\n",
    "                        else:\n",
    "                            word_cl1 = re.sub(r'[^\\w\\s]','',word)\n",
    "                            word_cl1 = word_cl1.lower()\n",
    "                            if word_cl1 not in stop:\n",
    "                                stemmed_word = ps.stem(word_cl1)\n",
    "                                important1.append(stemmed_word)\n",
    "                elif row[0] == 'Title':\n",
    "                    for word in row[1].strip().split():\n",
    "                        if word == 'NA':\n",
    "                            continue\n",
    "                        else:\n",
    "                            word_cl1 = re.sub(r'[^\\w\\s]','',word)\n",
    "                            word_cl1 = word_cl1.lower()\n",
    "                            if word_cl1 not in stop:\n",
    "                                stemmed_word = ps.stem(word_cl1)\n",
    "                                title.append(stemmed_word)\n",
    "\n",
    "\n",
    "        if len(intro_plot) < 1:\n",
    "            continue\n",
    "        else:\n",
    "            len_file = len(intro_plot) # the total number of words\n",
    "            word = Vocabulary[int(term_id)]\n",
    "            count = intro_plot.count(word) # the number of times that word is repeated in the document\n",
    "            tf_in = count/len_file # the frequency of the word in the document\n",
    "            idf = np.log(len_dict/len(val_word)) # the IDF \n",
    "\n",
    "            tf_title = 0\n",
    "            if len(title) > 0:\n",
    "                tf_title = title.count(word)/len(title)\n",
    "            tf_imp = important1.count(word)/len(important1)\n",
    "            if tf_title > 0:\n",
    "                tf = tf_in + tf_imp + (tf_title)**2\n",
    "\n",
    "                if term_id in New_Dict.keys():  # add the word to the new dictioary if the word is not already inside and add the new value if the word is already inside\n",
    "                    New_Dict[term_id].append((doc, tf*idf))\n",
    "                else:\n",
    "                    New_Dict[term_id] = [(doc, tf*idf)]\n",
    "            else:\n",
    "                if tf_imp > 0:\n",
    "                    tf = tf_in + tf_imp\n",
    "\n",
    "                    if term_id in New_Dict.keys():  # add the word to the new dictioary if the word is not already inside and add the new value if the word is already inside\n",
    "                        New_Dict[term_id].append((doc, tf*idf))\n",
    "                    else:\n",
    "                        New_Dict[term_id] = [(doc, tf*idf)]\n",
    "                else:\n",
    "                    tf = tf_in \n",
    "\n",
    "                    if term_id in New_Dict.keys():  # add the word to the new dictioary if the word is not already inside and add the new value if the word is already inside\n",
    "                        New_Dict[term_id].append((doc, tf*idf))\n",
    "                    else:\n",
    "                        New_Dict[term_id] = [(doc, tf*idf)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then using the same cose to compute the query's tdif, vectorization, cosine similarity and ranking, we could manage to rank our film with a better result respect to the second serching engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query TF-IDF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_query(query):\n",
    "    tf_idf = {}\n",
    "    len_query = len(query)\n",
    "    len_dict = len(os.listdir(path))\n",
    "    DF = {}\n",
    "    for word in query:# its a count funtion\n",
    "        if word not in DF.keys():\n",
    "            DF[word] = 1 \n",
    "        else:\n",
    "            DF[word]+=1\n",
    "    \n",
    "    for word in query:\n",
    "        \n",
    "        # TF = (Frequency of the word in the sentence) / (Total number of words in the sentence)\n",
    "        count = query.count(word)\n",
    "        freq = count/len_query\n",
    "        tf = freq\n",
    "        \n",
    "        val_word = DF[word]\n",
    "        idf = idf = np.log(len_dict/val_word)\n",
    "        if word in Vocabulary:\n",
    "            term_id = Vocabulary.index(word)\n",
    "            if word in tf_idf.keys():\n",
    "                tf_idf[str(term_id)]+= tf*idf\n",
    "            else:\n",
    "                tf_idf[str(term_id)] = tf*idf\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_query = tf_idf_query(query) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorization, Cosine Similarity and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_list = []\n",
    "for i in range(len(result)):\n",
    "    small_dict = {}\n",
    "    for word in New_Dict.keys():\n",
    "        for files in New_Dict[word]:\n",
    "            if files[0] == result[i]:\n",
    "                small_dict[word] = files[1]\n",
    "    result_list.append(small_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(vec1, vec2):\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "    \n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "cos_result = {}\n",
    "for i in range(len(result)):\n",
    "    cosine = get_cosine(tf_idf_query, result_list[i])\n",
    "    cos_result[result[i]] = cosine*100\n",
    "    \n",
    "cos_result = sorted(cos_result.items(), key=operator.itemgetter(1), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Intro</th>\n",
       "      <th>Url</th>\n",
       "      <th>Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>When in Rome</td>\n",
       "      <td>When in Rome is a 2010 American romantic comed...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/When_in_Rome_(20...</td>\n",
       "      <td>6.835857966671183 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear John</td>\n",
       "      <td>Dear John is a 2010 American romantic drama-wa...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Dear_John_(2010_...</td>\n",
       "      <td>5.704633376565299 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I Love You Phillip Morris</td>\n",
       "      <td>I Love You Phillip Morris is a 2009 black come...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/I_Love_You_Phill...</td>\n",
       "      <td>5.264481089373219 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My One and Only</td>\n",
       "      <td>My One and Only is a 2009 comedy-drama film lo...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/My_One_and_Only_...</td>\n",
       "      <td>4.689575381407523 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Never Let Me Go</td>\n",
       "      <td>Never Let Me Go is a 2010 British dystopian ro...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Never_Let_Me_Go_...</td>\n",
       "      <td>4.209033222443061 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Eat Pray Love</td>\n",
       "      <td>Eat Pray Love is a 2010 American biographical ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Eat_Pray_Love</td>\n",
       "      <td>3.918970377109158 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I Can Do Bad All by Myself</td>\n",
       "      <td>I Can Do Bad All by Myself is a 2009 romantic ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/I_Can_Do_Bad_All...</td>\n",
       "      <td>3.623062757513399 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Informers</td>\n",
       "      <td>The Informers is a 2008 American ensemble Holl...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Informers_(2...</td>\n",
       "      <td>3.2380426575731334 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Limits of Control</td>\n",
       "      <td>The Limits of Control is a 2009 American film ...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Limits_of_Co...</td>\n",
       "      <td>3.1846058945767446 %</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hurricane Season</td>\n",
       "      <td>Hurricane Season is a 2010 sports drama film d...</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Hurricane_Season...</td>\n",
       "      <td>3.1793814369053632 %</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Name  \\\n",
       "0                When in Rome   \n",
       "1                   Dear John   \n",
       "2   I Love You Phillip Morris   \n",
       "3             My One and Only   \n",
       "4             Never Let Me Go   \n",
       "5               Eat Pray Love   \n",
       "6  I Can Do Bad All by Myself   \n",
       "7               The Informers   \n",
       "8       The Limits of Control   \n",
       "9            Hurricane Season   \n",
       "\n",
       "                                               Intro  \\\n",
       "0  When in Rome is a 2010 American romantic comed...   \n",
       "1  Dear John is a 2010 American romantic drama-wa...   \n",
       "2  I Love You Phillip Morris is a 2009 black come...   \n",
       "3  My One and Only is a 2009 comedy-drama film lo...   \n",
       "4  Never Let Me Go is a 2010 British dystopian ro...   \n",
       "5  Eat Pray Love is a 2010 American biographical ...   \n",
       "6  I Can Do Bad All by Myself is a 2009 romantic ...   \n",
       "7  The Informers is a 2008 American ensemble Holl...   \n",
       "8  The Limits of Control is a 2009 American film ...   \n",
       "9  Hurricane Season is a 2010 sports drama film d...   \n",
       "\n",
       "                                                 Url            Similarity  \n",
       "0  https://en.wikipedia.org/wiki/When_in_Rome_(20...   6.835857966671183 %  \n",
       "1  https://en.wikipedia.org/wiki/Dear_John_(2010_...   5.704633376565299 %  \n",
       "2  https://en.wikipedia.org/wiki/I_Love_You_Phill...   5.264481089373219 %  \n",
       "3  https://en.wikipedia.org/wiki/My_One_and_Only_...   4.689575381407523 %  \n",
       "4  https://en.wikipedia.org/wiki/Never_Let_Me_Go_...   4.209033222443061 %  \n",
       "5        https://en.wikipedia.org/wiki/Eat_Pray_Love   3.918970377109158 %  \n",
       "6  https://en.wikipedia.org/wiki/I_Can_Do_Bad_All...   3.623062757513399 %  \n",
       "7  https://en.wikipedia.org/wiki/The_Informers_(2...  3.2380426575731334 %  \n",
       "8  https://en.wikipedia.org/wiki/The_Limits_of_Co...  3.1846058945767446 %  \n",
       "9  https://en.wikipedia.org/wiki/Hurricane_Season...  3.1793814369053632 %  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_df = []\n",
    "for movie in cos_result:\n",
    "    element_in_movie = {}\n",
    "    with open(path+movie[0]) as fd:\n",
    "        rd = csv.reader(fd, delimiter=\"\\t\", quotechar='\"')\n",
    "        for row in rd:\n",
    "            if row[0] == 'Name':\n",
    "                element_in_movie['Name'] = row[1]\n",
    "            elif row[0] == 'Intro':\n",
    "                element_in_movie['Intro'] = row[1]\n",
    "            elif row[0] == 'Url':\n",
    "                element_in_movie['Url'] = row[1]\n",
    "        element_in_movie['Similarity'] = str(movie[1])+' %'\n",
    "    to_df.append(element_in_movie)\n",
    "df = pd.DataFrame(to_df)\n",
    "df = df.reindex(('Name', 'Intro', 'Url', 'Similarity'), axis=1)\n",
    "df.iloc[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
